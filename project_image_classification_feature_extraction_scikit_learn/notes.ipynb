{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a97e1c",
   "metadata": {},
   "source": [
    "**Feature Extraction:** https://github.com/christiansafka/img2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca70e722",
   "metadata": {},
   "source": [
    "# **img2vec_pytorch**\n",
    "\n",
    "### **1. The Core Concept**\n",
    "\n",
    "`img2vec` is a tool that translates an **Image** (pixels) into a **Vector** (a list of numbers).\n",
    "\n",
    "* **Input:** A raw image (e.g., a photo of a sunrise).\n",
    "* **The Black Box:** A pre-trained Deep Learning model (usually ResNet) that already knows how to \"see\" shapes, colors, and objects.\n",
    "* **Output:** A feature vector (e.g., a list of 512 numbers) that represents the *content* of that image.\n",
    "\n",
    "### **2. Why use it?**\n",
    "\n",
    "Standard Machine Learning models (like Random Forest or SVM) cannot understand raw images because they are just massive grids of pixels.\n",
    "\n",
    "* **Without img2vec:** You have to build and train a complex Convolutional Neural Network (CNN) from scratch (like YOLO or Teachable Machine).\n",
    "* **With img2vec:** You extract the \"smart\" features instantly and feed them into a simple, fast classifier like Random Forest. It essentially allows you to do \"Deep Learning\" results with \"Standard ML\" speed and simplicity.\n",
    "\n",
    "### **3. The Workflow (As seen in the tutorial)**\n",
    "\n",
    "1. **Load Image:** Open the file using PIL (Python Imaging Library).\n",
    "2. **Extract:** Pass the image to `img2vec.get_vec()`. This runs the image through the pre-trained neural network and captures the output before the final classification layer.\n",
    "3. **Train:** Train a standard classifier (Random Forest) on these vectors.\n",
    "\n",
    "### **4. The Code Cheatsheet**\n",
    "\n",
    "Here is the essential code pattern extracted from the video:\n",
    "\n",
    "```python\n",
    "# 1. Setup\n",
    "from img2vec_pytorch import Image2Vec\n",
    "from PIL import Image\n",
    "\n",
    "# Initialize the model (downloads a pre-trained ResNet model)\n",
    "img2vec = Image2Vec() \n",
    "\n",
    "# 2. Convert Image to Vector\n",
    "img = Image.open('./weather_dataset/rain/rain1.jpg')\n",
    "feature_vector = img2vec.get_vec(img) \n",
    "\n",
    "# feature_vector is now a list of numbers representing the image\n",
    "# You can now feed this into sklearn models\n",
    "\n",
    "```\n",
    "\n",
    "### **5. Performance Context**\n",
    "\n",
    "In the video, this method achieved **94.4% accuracy** on the weather dataset. This demonstrates that using `img2vec` to extract features and a simple Random Forest to classify them is a highly effective strategy for standard image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722eef24",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# **Notes**\n",
    "\n",
    "**Project Overview and Setup**\n",
    "\n",
    "* **Goal:** Build an image classifier that utilizes feature extraction to categorize images.\n",
    "* **Dataset:** Uses a weather dataset with four categories: `cloudy`, `rain`, `shine`, and `sunrise`.\n",
    "    * The data is split into `train` (training) and `val` (validation) directories.\n",
    "    * This is the same dataset used in previous YOLOv8 and Teachable Machine tutorials.\n",
    "\n",
    "\n",
    "* **Environment:** The project is created in PyCharm using a virtual environment with Python 3.8.\n",
    "* **Requirements:**\n",
    "    * `img2vec_pytorch`: A library for feature extraction using pre-trained models.\n",
    "    * `scikit-learn`: For the classifier (Random Forest) and metrics.\n",
    "    * `pillow` (PIL): For image handling.\n",
    "\n",
    "\n",
    "\n",
    "**Feature Extraction Implementation (`main.py`)**\n",
    "\n",
    "* **Initialization:** Import `Image2Vec` and create a feature extractor object.\n",
    "* **Data Preparation:**\n",
    "    * Define paths for `train` and `val` directories.\n",
    "    * Iterate through both directories and their subcategories (labels).\n",
    "    * Load each image using `Image.open()` from PIL.\n",
    "    * Extract features from the image using `img2vec.get_vec(image)`.\n",
    "    * Append features and corresponding labels (directory names) to lists.\n",
    "\n",
    "\n",
    "* **Data Organization:** Store training and validation data (features and labels) into a dictionary for easy access.\n",
    "\n",
    "**Model Training and Evaluation**\n",
    "\n",
    "* **Classifier:** Uses `RandomForestClassifier` from `sklearn.ensemble`.\n",
    "    * Note: While Random Forest is used for its robustness and ease of use, other classifiers from scikit-learn could also be used.\n",
    "\n",
    "\n",
    "* **Training:** Initialize the model and train it using `model.fit()` with the training features and training labels.\n",
    "* **Testing:**\n",
    "    * Generate predictions on the unseen validation set using `model.predict()`.\n",
    "    * Calculate accuracy using `accuracy_score` from `sklearn.metrics` comparing predictions to validation labels.\n",
    "\n",
    "\n",
    "* **Results:** The model achieved an accuracy of 94.4%.\n",
    "\n",
    "**Saving the Model**\n",
    "\n",
    "* **Method:** Use the `pickle` library to serialize and save the trained model to a file named `model.p`.\n",
    "\n",
    "**Inference (`infer.py`)**\n",
    "\n",
    "* **Setup:** Create a separate script to load the saved model and predict new images.\n",
    "* **Process:**\n",
    "    1. Initialize the `Image2Vec` extractor.\n",
    "    2. Load a target image (e.g., `cloudy4.jpg`) using PIL.\n",
    "    3. Extract features from the new image using `get_vec`.\n",
    "    4. Load the trained model from `model.p` using `pickle.load()`.\n",
    "    5. Predict the category using `model.predict()` with the extracted features.\n",
    "\n",
    "\n",
    "* **Result:** The script correctly predicted the category \"cloudy\" for the test image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7560fc1",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# **SVC only vs. SVC with feature extraction first**\n",
    "\n",
    "The difference is massive. It is the difference between **memorizing pixels** versus **understanding content**.\n",
    "\n",
    "Here is the breakdown of why the \"Feature Extraction\" method (Current Project) destroys the \"Raw Pixel\" method (Previous Parking Project) on real-world data.\n",
    "\n",
    "### 1. The \"Raw Pixel\" Problem (Previous Project)\n",
    "\n",
    "In your parking slot project, you resized images to **15x15** pixels and flattened them.\n",
    "\n",
    "* **How it works:** The model looks at pixel #1 and asks \"Is it gray?\" It looks at pixel #50 and asks \"Is it red?\"\n",
    "* **The Flaw:** It is **position dependent**. If you take a picture of a car, and then shift the camera 2 inches to the left, **every single pixel value changes**. The model will likely fail because it hasn't memorized this specific arrangement of pixels.\n",
    "* **Why it worked before:** The parking camera was **fixed**. The cars were always in the exact same spot, at the exact same size.\n",
    "\n",
    "### 2. The \"Feature Extraction\" Advantage (Current Project)\n",
    "\n",
    "In this project, `img2vec` uses a pre-trained brain (ResNet-18) to look at the image *before* the classifier sees it.\n",
    "\n",
    "* **How it works:** The neural network doesn't care about specific pixel positions. It scans the image and outputs a summary like: *\"I see a sharp edge, a metallic texture, and a cylinder shape.\"*\n",
    "* **The Vector:** That list of 512 numbers you get isn't pixelsâ€”it's a list of **concepts** (textures, shapes, patterns).\n",
    "* **The Result:** If you take a picture of a \"Glass Bottle\" and rotate it, zoom in, or move it to the corner, the **concepts** (glass texture, bottle shape) stay the same. The Random Forest classifies these concepts, not the pixels.\n",
    "\n",
    "### Summary Comparison\n",
    "\n",
    "| Feature | Raw Pixels (Old Method) | Feature Extraction (New Method) |\n",
    "| --- | --- | --- |\n",
    "| **Input** | Raw colors (Red=255, Green=0...) | Abstract concepts (Shape, Texture, Edges) |\n",
    "| **Rotation/Movement** | Fails immediately (Fragile) | Works fine (Robust) |\n",
    "| **Image Size** | Must be tiny (15x15) to run fast | Can be large (HD) and still fast |\n",
    "| **Intelligence** | \"There is a red dot at coordinate 10,10\" | \"There is a shiny object in the image\" |\n",
    "\n",
    "**In short:**\n",
    "\n",
    "* **Method 1 (Raw Pixels):** Like trying to identify a book by measuring exactly where the ink dots are on the page.\n",
    "* **Method 2 (Feature Extraction):** Like identifying a book by reading the plot summary."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
