{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f173e118",
   "metadata": {},
   "source": [
    "# **MediaPipe**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ecb77",
   "metadata": {},
   "source": [
    "**Refrence:** https://ai.google.dev/edge/mediapipe/solutions/guide\n",
    "\n",
    "**MediaPipe** is an open-source framework developed by Google that provides pre-built, high-performance machine learning solutions for computer vision.\n",
    "\n",
    "Think of it this way:\n",
    "\n",
    "* **OpenCV** gives you the \"tools\" to process images (blur, resize, threshold).\n",
    "* **MediaPipe** gives you **complete, pre-trained pipelines** to understand what is *in* the image (hands, faces, poses).\n",
    "\n",
    "### Key Features\n",
    "\n",
    "It is famous for being **extremely fast** (real-time) even on mobile devices and CPUs because it is highly optimized.\n",
    "\n",
    "Common solutions include:\n",
    "\n",
    "* **Hands:** Tracks 21 3D landmarks on a hand (knuckles, fingertips).\n",
    "* **Pose:** Tracks full-body skeletal points (shoulders, elbows, knees).\n",
    "* **Face Mesh:** Maps 468 points on a face (lips, eyes, eyebrows).\n",
    "* **Selfie Segmentation:** Separates a person from the background (like Zoom background blur).\n",
    "\n",
    "### How it is used in a CV Project\n",
    "\n",
    "You typically use **OpenCV** to capture the video, and **MediaPipe** to analyze it.\n",
    "\n",
    "**The Workflow:**\n",
    "\n",
    "1. **Capture:** Use OpenCV (`cv2.VideoCapture`) to get a frame.\n",
    "2. **Color Convert:** MediaPipe expects **RGB** images, but OpenCV loads in **BGR**, so you must convert it (`cv2.cvtColor`).\n",
    "3. **Process:** Pass the image to a MediaPipe solution (e.g., `hands.process(image)`).\n",
    "4. **Extract Data:** MediaPipe returns **\"Landmarks\"** (x, y, z coordinates).\n",
    "* Example: \"Index finger tip is at x: 0.5, y: 0.3\".\n",
    "\n",
    "\n",
    "5. **Draw:** You can use these coordinates to draw lines or control a game/mouse.\n",
    "\n",
    "**Simple Logic Example:**\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# 1. Initialize\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# 2. Capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# 3. Process\n",
    "rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "results = hands.process(rgb_frame)\n",
    "\n",
    "# 4. Use Results\n",
    "if results.multi_hand_landmarks:\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        # Get coordinates of the index finger tip (Point 8)\n",
    "        print(hand_landmarks.landmark[8]) \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e807211",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### 1. The Coding Style (`with ... as`)\n",
    "\n",
    "This is called a **Context Manager**.\n",
    "\n",
    "* **What it does:** It automatically handles \"cleanup\" for you.\n",
    "* **Why use it:** When the code block finishes (or if it crashes), Python automatically closes the `face_detection` model to free up memory. It's safer than manually opening and closing it.\n",
    "\n",
    "### 2. The Parameters\n",
    "\n",
    "* **`model_selection=1` (Range Setting):**\n",
    "* **0:** Use for faces **close** to the camera (within 2 meters, e.g., phone selfies).\n",
    "* **1:** Use for faces **far** from the camera (2 to 5 meters, e.g., full-body shots).\n",
    "\n",
    "\n",
    "* **`min_detection_confidence=0.5` (Strictness):**\n",
    "* This is the \"Trust Threshold.\"\n",
    "* **0.5 (50%):** The AI must be at least 50% sure it found a face before showing it to you.\n",
    "* *Higher (e.g., 0.9):* Very strict, fewer mistakes, but might miss some faces.\n",
    "* *Lower (e.g., 0.1):* Very loose, catches everything, but might think a lamp is a face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd797b",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# **MediaPipe on Windows Fix Guide**\n",
    "\n",
    "### **The \"MediaPipe on Windows\" Fix Guide**\n",
    "\n",
    "**Step 0: The File Name Rule (Critical)**\n",
    "\n",
    "Before doing anything, make sure you **do not** have any file named `mediapipe.py` in your folder. If you do, rename it to `my_test.py`.\n",
    "\n",
    "**Step 1: Force Python 3.11**\n",
    "\n",
    "Run this command to switch `uv` away from Python 3.12 (which breaks MediaPipe).\n",
    "\n",
    "Bash\n",
    "\n",
    "```\n",
    "uv python pin 3.11\n",
    "```\n",
    "\n",
    "**Step 2: Install The \"Golden Combo\"**\n",
    "\n",
    "Run this **single command** to install all the compatible library versions at once. This prevents the \"Dependency Hell\" errors.\n",
    "\n",
    "Bash\n",
    "\n",
    "```\n",
    "uv add \"numpy<2\" \"opencv-python<4.10\" \"protobuf<4\" \"mediapipe==0.10.9\"\n",
    "```\n",
    "\n",
    "- **Why this works:**\n",
    "    \n",
    "    - `numpy<2`: MediaPipe crashes on the new NumPy 2.0.\n",
    "        \n",
    "    - `opencv-python<4.10`: Old OpenCV is needed to work with old NumPy.\n",
    "        \n",
    "    - `protobuf<4`: MediaPipe needs the old Protobuf 3 to talk to C++.\n",
    "        \n",
    "    - `mediapipe==0.10.9`: The most stable version for Windows right now.\n",
    "        \n",
    "\n",
    "**Step 3: Sync & Run**\n",
    "\n",
    "Finalize the setup and run the code.\n",
    "\n",
    "Bash\n",
    "\n",
    "```\n",
    "uv sync\n",
    "uv run image.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
