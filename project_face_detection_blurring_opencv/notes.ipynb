{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f173e118",
   "metadata": {},
   "source": [
    "# **MediaPipe**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9ecb77",
   "metadata": {},
   "source": [
    "**Refrence:** https://ai.google.dev/edge/mediapipe/solutions/guide\n",
    "\n",
    "**MediaPipe** is an open-source framework developed by Google that provides pre-built, high-performance machine learning solutions for computer vision.\n",
    "\n",
    "Think of it this way:\n",
    "\n",
    "* **OpenCV** gives you the \"tools\" to process images (blur, resize, threshold).\n",
    "* **MediaPipe** gives you **complete, pre-trained pipelines** to understand what is *in* the image (hands, faces, poses).\n",
    "\n",
    "### Key Features\n",
    "\n",
    "It is famous for being **extremely fast** (real-time) even on mobile devices and CPUs because it is highly optimized.\n",
    "\n",
    "Common solutions include:\n",
    "\n",
    "* **Hands:** Tracks 21 3D landmarks on a hand (knuckles, fingertips).\n",
    "* **Pose:** Tracks full-body skeletal points (shoulders, elbows, knees).\n",
    "* **Face Mesh:** Maps 468 points on a face (lips, eyes, eyebrows).\n",
    "* **Selfie Segmentation:** Separates a person from the background (like Zoom background blur).\n",
    "\n",
    "### How it is used in a CV Project\n",
    "\n",
    "You typically use **OpenCV** to capture the video, and **MediaPipe** to analyze it.\n",
    "\n",
    "**The Workflow:**\n",
    "\n",
    "1. **Capture:** Use OpenCV (`cv2.VideoCapture`) to get a frame.\n",
    "2. **Color Convert:** MediaPipe expects **RGB** images, but OpenCV loads in **BGR**, so you must convert it (`cv2.cvtColor`).\n",
    "3. **Process:** Pass the image to a MediaPipe solution (e.g., `hands.process(image)`).\n",
    "4. **Extract Data:** MediaPipe returns **\"Landmarks\"** (x, y, z coordinates).\n",
    "* Example: \"Index finger tip is at x: 0.5, y: 0.3\".\n",
    "\n",
    "\n",
    "5. **Draw:** You can use these coordinates to draw lines or control a game/mouse.\n",
    "\n",
    "**Simple Logic Example:**\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# 1. Initialize\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands()\n",
    "\n",
    "# 2. Capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# 3. Process\n",
    "rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "results = hands.process(rgb_frame)\n",
    "\n",
    "# 4. Use Results\n",
    "if results.multi_hand_landmarks:\n",
    "    for hand_landmarks in results.multi_hand_landmarks:\n",
    "        # Get coordinates of the index finger tip (Point 8)\n",
    "        print(hand_landmarks.landmark[8]) \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e807211",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### 1. The Coding Style (`with ... as`)\n",
    "\n",
    "This is called a **Context Manager**.\n",
    "\n",
    "* **What it does:** It automatically handles \"cleanup\" for you.\n",
    "* **Why use it:** When the code block finishes (or if it crashes), Python automatically closes the `face_detection` model to free up memory. It's safer than manually opening and closing it.\n",
    "\n",
    "### 2. The Parameters\n",
    "\n",
    "* **`model_selection=1` (Range Setting):**\n",
    "* **0:** Use for faces **close** to the camera (within 2 meters, e.g., phone selfies).\n",
    "* **1:** Use for faces **far** from the camera (2 to 5 meters, e.g., full-body shots).\n",
    "\n",
    "\n",
    "* **`min_detection_confidence=0.5` (Strictness):**\n",
    "* This is the \"Trust Threshold.\"\n",
    "* **0.5 (50%):** The AI must be at least 50% sure it found a face before showing it to you.\n",
    "* *Higher (e.g., 0.9):* Very strict, fewer mistakes, but might miss some faces.\n",
    "* *Lower (e.g., 0.1):* Very loose, catches everything, but might think a lamp is a face."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd797b",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# **MediaPipe on Windows Fix Guide**\n",
    "\n",
    "### **The \"MediaPipe on Windows\" Fix Guide**\n",
    "\n",
    "**Step 0: The File Name Rule (Critical)**\n",
    "\n",
    "Before doing anything, make sure you **do not** have any file named `mediapipe.py` in your folder. If you do, rename it to `my_test.py`.\n",
    "\n",
    "**Step 1: Force Python 3.11**\n",
    "\n",
    "Run this command to switch `uv` away from Python 3.12 (which breaks MediaPipe).\n",
    "\n",
    "Bash\n",
    "\n",
    "```\n",
    "uv python pin 3.11\n",
    "```\n",
    "\n",
    "**Step 2: Install The \"Golden Combo\"**\n",
    "\n",
    "Run this **single command** to install all the compatible library versions at once. This prevents the \"Dependency Hell\" errors.\n",
    "\n",
    "Bash\n",
    "\n",
    "```\n",
    "uv add \"numpy<2\" \"opencv-python<4.10\" \"protobuf<4\" \"mediapipe==0.10.9\"\n",
    "```\n",
    "\n",
    "- **Why this works:**\n",
    "    \n",
    "    - `numpy<2`: MediaPipe crashes on the new NumPy 2.0.\n",
    "        \n",
    "    - `opencv-python<4.10`: Old OpenCV is needed to work with old NumPy.\n",
    "        \n",
    "    - `protobuf<4`: MediaPipe needs the old Protobuf 3 to talk to C++.\n",
    "        \n",
    "    - `mediapipe==0.10.9`: The most stable version for Windows right now.\n",
    "        \n",
    "\n",
    "**Step 3: Sync & Run**\n",
    "\n",
    "Finalize the setup and run the code.\n",
    "\n",
    "Bash\n",
    "\n",
    "```\n",
    "uv sync\n",
    "uv run image.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d925d2",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# **relative to absolute conversion**\n",
    "\n",
    "This step converts **Percentages** (0.0 to 1.0) into **Pixels** (0 to 1920, etc.).\n",
    "\n",
    "Here is the breakdown:\n",
    "\n",
    "1. **MediaPipe speaks in Percentages (Relative):**\n",
    "* It returns values like `xmin = 0.5`.\n",
    "* This means: \"The face starts at **50%** of the image width.\"\n",
    "* It doesn't know (or care) how big your image actually is in pixels.\n",
    "\n",
    "\n",
    "2. **OpenCV needs Pixels (Absolute):**\n",
    "* To draw a rectangle, OpenCV needs exact numbers like `x = 960`.\n",
    "* It cannot draw at coordinate \"0.5\".\n",
    "\n",
    "\n",
    "3. **The Math:**\n",
    "* `x1 * W`  `0.5 * 1920` = **960 pixels**.\n",
    "* `int(...)`  Rounds the decimal (e.g., 960.4) to a whole number (960) because pixels must be integers.\n",
    "\n",
    "\n",
    "\n",
    "**In short:** You are translating \"The middle of the screen\" into \"Pixel number 960.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699038e3",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# **argparse**\n",
    "\n",
    "`argparse` is a built-in Python library used to write **command-line interfaces (CLIs)**.\n",
    "\n",
    "In simple terms, it lets you control your code from the terminal (command prompt) without changing the code itself.\n",
    "\n",
    "### Why do we use it?\n",
    "\n",
    "Instead of hard-coding variables like `mode = 'video'` or changing the file path inside the script every time you want to run a different video, you can pass them as \"arguments\" when you run the command.\n",
    "\n",
    "**Example without argparse:**\n",
    "You have to open the file, find line 10, change `img_path = \"photo.jpg\"` to `video.mp4`, save, and run.\n",
    "\n",
    "**Example with argparse:**\n",
    "You just type this in the terminal:\n",
    "\n",
    "```bash\n",
    "python main.py --mode video --filePath my_video.mp4\n",
    "\n",
    "```\n",
    "\n",
    "### Key Features in Your Code\n",
    "\n",
    "1. **`argparse.ArgumentParser()`**: This creates the \"container\" that listens for commands from the terminal.\n",
    "2. **`args.add_argument(...)`**: This defines the rules.\n",
    "* `--mode`: You defined a flag named \"mode\".\n",
    "* `default='webcam'`: If the user doesn't type anything, the code assumes they want to use the webcam.\n",
    "\n",
    "\n",
    "3. **`args.parse_args()`**: This is the \"Go\" button. It reads what the user typed in the terminal and stores the answers in the `args` variable.\n",
    "\n",
    "### How it works in your project\n",
    "\n",
    "Because of these lines:\n",
    "\n",
    "```python\n",
    "args.add_argument(\"--mode\", default='webcam')\n",
    "args.add_argument(\"--filePath\", default=None)\n",
    "\n",
    "```\n",
    "\n",
    "You can now run your script in three different ways without changing a single line of code:\n",
    "\n",
    "* **To run Webcam:**\n",
    "`python main.py` (It uses the default 'webcam')\n",
    "* **To run an Image:**\n",
    "`python main.py --mode image --filePath ./assets/photo.jpg`\n",
    "* **To run a Video:**\n",
    "`python main.py --mode video --filePath ./assets/video.mp4`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
