{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "472c3aec",
   "metadata": {},
   "source": [
    "### **Project Overview & Requirements**\n",
    "\n",
    "* **Goal:** Build a simple, robust image classifier using Python to categorize images into \"empty\" or \"not empty\".\n",
    "* **Process:** The workflow consists of four steps: Data Preparation, Data Splitting, Training, and Testing.\n",
    "* **Libraries Used:**\n",
    "    * `scikit-learn`: For machine learning models and utilities.\n",
    "    * `scikit-image`: For image processing (loading and resizing).\n",
    "    * `numpy`: For array manipulation.\n",
    "\n",
    "\n",
    "* **Data Source:** Images come from a parking slot detector project. The \"not empty\" category contains cars, while the \"empty\" category contains empty parking spots.\n",
    "* **Applicability:** This classifier works best for simple problems where categories are visually distinct, rather than complex state-of-the-art challenges.\n",
    "\n",
    "### **Step 1: Data Preparation**\n",
    "\n",
    "* **Setup:** Define input directories and categories (`empty`, `not_empty`).\n",
    "* **Data Structures:** Create lists for `data` (image features) and `labels` (category indices).\n",
    "* **Image Processing Loop:**\n",
    "    * Iterate through all files in the directories using `os.listdir`.\n",
    "    * Read images using `imread` from `skimage.io`.\n",
    "    * **Resizing:** Resize all images to a uniform 15x15 resolution using `skimage.transform.resize`.\n",
    "    * **Flattening:** Convert the image matrix (RGB) into a flat, 1D array using `.flatten()` before appending to the `data` list. Classifiers require 1D arrays as input.\n",
    "    * **Labeling:** Append the corresponding category index to the `labels` list.\n",
    "\n",
    "\n",
    "* **Conversion:** Convert the `data` and `labels` lists into Numpy arrays (`np.asarray`).\n",
    "\n",
    "### **Step 2: Data Splitting**\n",
    "\n",
    "* **Function:** Use `train_test_split` from `sklearn.model_selection` to divide data into training and testing sets.\n",
    "* **Parameters:**\n",
    "    * `test_size=0.2`: Allocates 20% of the data for testing and 80% for training.\n",
    "    * `shuffle=True`: Randomizes the data order to remove bias.\n",
    "    * `stratify=labels`: Ensures the proportion of categories in the splits matches the original dataset.\n",
    "\n",
    "\n",
    "\n",
    "### **Step 3: Training the Classifier**\n",
    "\n",
    "* **Model:** Use a Support Vector Classifier (`SVC`) from `sklearn.svm`.\n",
    "* **Grid Search:** Implement `GridSearchCV` to test multiple hyperparameter combinations automatically.\n",
    "* **Hyperparameters:**\n",
    "    * `gamma`: Tested values `[0.01, 0.001, 0.0001]`.\n",
    "    * `C`: Tested values `[1, 10, 100, 1000]`.\n",
    "\n",
    "\n",
    "* **Execution:** This setup trains 12 different classifiers (3 gammas Ã— 4 Cs) to find the optimal combination.\n",
    "* **Training:** Call `.fit(x_train, y_train)` on the grid search object to execute the training.\n",
    "\n",
    "### **Step 4: Testing and Saving**\n",
    "\n",
    "* **Selection:** Retrieve the best-performing model using `grid_search.best_estimator_`.\n",
    "* **Prediction:** Use `.predict(x_test)` on the test set to generate classification predictions.\n",
    "* **Evaluation:** Calculate the accuracy using `accuracy_score` from `sklearn.metrics`.\n",
    "    * *Result:* The model achieved approximately 99.9% accuracy.\n",
    "\n",
    "\n",
    "* **Saving:** Use the `pickle` library to save the trained model.\n",
    "    * Use `pickle.dump` to write the model to a file named `model.p` for future use in other projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec2094f",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# **Suppport Vector Classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb2644e",
   "metadata": {},
   "source": [
    "**SVC (Support Vector Classifier) Crash Course**\n",
    "\n",
    "### 1. The Core Concept: \"The Widest Street\"\n",
    "\n",
    "Imagine you have red balls (cars) and blue balls (empty spots) thrown on the floor. Your job is to place a stick on the floor that perfectly separates them.\n",
    "\n",
    "* **The Problem:** You could place the stick in many different angles and it would still separate them. Which one is best?\n",
    "* **The SVC Solution:** SVC tries to find the position for the stick that leaves the **widest possible gap** (or street) between the red balls and the blue balls.\n",
    "* This \"stick\" is called the **Hyperplane**.\n",
    "* The \"gap\" is called the **Margin**.\n",
    "* The specific balls that touch the edge of the street are called the **Support Vectors** (hence the name). They are the only data points the model actually cares about; the rest are safe behind the line.\n",
    "\n",
    "\n",
    "\n",
    "### 2. The \"Knobs\" You Tuned (C and Gamma)\n",
    "\n",
    "In the video, the code didn't just pick one setting; it used **Grid Search** to test different combinations of `C` and `Gamma`. Think of these as two knobs that control the \"shape\" of your stick.\n",
    "\n",
    "#### **Knob 1: `C` (The Strictness)**\n",
    "\n",
    "This controls how much you punish the model for making a mistake on the training data.\n",
    "\n",
    "* **High C:** \"I want 0 mistakes.\" The model will create a crazy, jagged boundary just to make sure every single training point is on the correct side. (Risk: Overfitting).\n",
    "* **Low C:** \"Chill out.\" The model accepts a few mistakes if it means keeping the boundary straight and simple. (Better for generalization).\n",
    "* *In the video:* You tested `[1, 10, 100, 1000]` to see how strict the model should be.\n",
    "\n",
    "#### **Knob 2: `Gamma` (The Curvature)**\n",
    "\n",
    "This controls how far the influence of a single training example reaches.\n",
    "\n",
    "* **High Gamma:** Only points very close to the boundary matter. This creates \"islands\" around specific points (a very curvy, complex boundary).\n",
    "* **Low Gamma:** Points far away still have influence. This creates a smoother, gentler boundary.\n",
    "* *In the video:* You tested `[0.01, 0.001, 0.0001]` to see how \"curvy\" the boundary needed to be.\n",
    "\n",
    "### 3. Why did it work so well (99.9%)?\n",
    "\n",
    "The video mentioned this classifier is robust for \"visually super distinct\" categories.\n",
    "\n",
    "* **Empty Spots** are gray/uniform.\n",
    "* **Cars** are colorful/complex.\n",
    "In the mathematical \"space\" SVC looks at, these two groups are likely very far apart. It didn't need a complex jagged line; a simple \"wide street\" was enough to separate them perfectly.\n",
    "\n",
    "### Summary\n",
    "\n",
    "* **SVC** draws a line between your classes.\n",
    "* It tries to make the **margin** (gap) as wide as possible.\n",
    "* **C** and **Gamma** control how straight or wiggly that line is allowed to be.\n",
    "* **Grid Search** simply automated the process of twisting these knobs until it found the perfect setting (the \"Best Estimator\").\n",
    "\n",
    "---\n",
    "\n",
    "## **SVM VS. SVC**\n",
    "\n",
    "They are **the same thing**, but with a tiny technical distinction.\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "* **SVM (Support Vector Machine):** This is the name of the **theory** or the entire mathematical concept. It covers everything: classification (grouping things), regression (predicting numbers), and outlier detection.\n",
    "* **SVC (Support Vector Classification):** This is the name of the **specific tool** inside the library (`scikit-learn`) that implements the SVM theory specifically for **Classification** tasks (like your \"Empty vs. Not Empty\" problem).\n",
    "\n",
    "**In short:**\n",
    "You are using an **SVM** (the concept) to solve your problem, and the tool you use to do it is called **SVC** (the code).\n",
    "\n",
    "**Bonus Note:**\n",
    "If you were trying to predict a continuous number (like the *price* of the car instead of whether it is there or not), you would use **SVR** (Support Vector Regression). Both SVC and SVR are types of SVMs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
