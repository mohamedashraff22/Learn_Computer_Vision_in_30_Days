{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38c3c905",
   "metadata": {},
   "source": [
    "**Dateset used:** https://www.kaggle.com/datasets/sudarshanvaidya/random-images-for-face-emotion-recognition?resource=download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e525ea",
   "metadata": {},
   "source": [
    "# **Notes**\n",
    "\n",
    "### **Project Overview & Data**\n",
    "\n",
    "* **Goal:** Build a real-time emotion classifier to detect \"Happy,\" \"Sad,\" or \"Surprised\" expressions using a webcam.\n",
    "* **Data Source:** https://www.kaggle.com/datasets/sudarshanvaidya/random-images-for-face-emotion-recognition?resource=download\n",
    "* **Tools:** Python, `scikit-learn` (Machine Learning), `mediapipe` (Feature Extraction), and `opencv` (Image/Video handling).\n",
    "\n",
    "### **Step 1: Data Cleaning**\n",
    "\n",
    "* **Category Reduction:** The \"Angry\" category was removed because it looked too similar to \"Sad,\" making classification difficult. The final categories are Happy, Sad, and Surprised.\n",
    "* **Quality Control:** Images were manually reviewed to remove those where the person was not looking at the camera (non-frontal) or did not clearly display the target emotion.\n",
    "* **Balancing:** The dataset was balanced so every category has exactly 96 images (matching the smallest category, \"Surprised\") to prevent bias.\n",
    "\n",
    "### **Step 2: Data Preparation (Feature Extraction)**\n",
    "\n",
    "* **Technique:** Instead of using raw pixels, the project uses **Face Landmarks** extracted via MediaPipe.\n",
    "* **Function:** A utility function `get_face_landmarks` converts an image into a list of 1,404 values (X, Y, Z coordinates for 468 landmarks).\n",
    "    * *Normalization:* To ensure the model works regardless of where the face is in the frame, the function subtracts the minimum coordinate values, effectively centering the data.\n",
    "\n",
    "\n",
    "* **Processing:**\n",
    "    * Iterate through all image categories.\n",
    "    * Convert images to RGB and extract landmarks.\n",
    "    * Append the landmarks and the corresponding emotion label (index) to a list.\n",
    "    * Save the processed data to a text file named `data.txt` using `numpy`.\n",
    "\n",
    "\n",
    "\n",
    "### **Step 3: Training the Model**\n",
    "\n",
    "* **Model Architecture:** A Random Forest Classifier from `scikit-learn` is used.\n",
    "* **Code Generation:** ChatGPT was used to generate the boilerplate training code.\n",
    "* **Process:**\n",
    "    * Load data from `data.txt`.\n",
    "    * Split data into Training and Testing sets using `stratify=labels` (to maintain category proportions) and `shuffle=True`.\n",
    "    * Train the model using `.fit()`.\n",
    "\n",
    "\n",
    "* **Results:** The model achieved approximately **77% accuracy**, with \"Happy\" being the most accurate category.\n",
    "* **Saving:** The trained model is saved as `model.p` using the `pickle` library.\n",
    "\n",
    "### **Step 4: Inference (Webcam Test)**\n",
    "\n",
    "* **Setup:** Use `cv2.VideoCapture` to read frames from the webcam in a loop.\n",
    "* **Real-time Processing:**\n",
    "    1. Load the saved model using `pickle.load`.\n",
    "    2. Extract face landmarks from the current webcam frame (setting `static_image_mode=False`).\n",
    "    3. Pass the landmarks to `model.predict` to get the emotion.\n",
    "\n",
    "\n",
    "* **Visualization:**\n",
    "    * Use `cv2.putText` to display the predicted emotion (Happy, Sad, or Surprised) in green text on the video feed.\n",
    "    * Set `draw=True` in the landmark utility to visually plot the face mesh on the user's face."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
