{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe94c621",
   "metadata": {},
   "source": [
    "**Data used:** https://www.kaggle.com/datasets/grassknoted/asl-alphabet/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44df24b8",
   "metadata": {},
   "source": [
    "# **Notes**\n",
    "\n",
    "\n",
    "### **Project Overview**\n",
    "\n",
    "* **Goal:** Build a real-time sign language detector for American Sign Language (ASL) letters 'A', 'B', and 'L'.\n",
    "* **Tech Stack:** The project uses Python with OpenCV, MediaPipe, and Scikit-learn.\n",
    "* **Workflow:** The process follows a standard three-step machine learning pipeline: Data Collection, Model Training, and Testing.\n",
    "\n",
    "### **Step 1: Data Collection**\n",
    "\n",
    "* **Setup:** A custom script captures frames from the webcam to build the dataset.\n",
    "* **Method:** The user records 100 distinct frames for each letter (A, B, L) by moving their hand in various positions (towards and away from the camera) to create diverse samples.\n",
    "* **Organization:** Images are stored in directories named '0', '1', and '2', representing the three encoded classes.\n",
    "\n",
    "### **Step 2: Strategy and Data Processing**\n",
    "\n",
    "* **Approach Selection:** The narrator rejects classifying the entire image or a cropped image in favor of **landmark detection**.\n",
    "* **Reasoning:** Extracting landmarks reduces dimensionality (converting an image of pixels into an array of ~21 points) and focuses solely on hand posture rather than background noise.\n",
    "* **Extraction:**\n",
    "    * Images are read and converted from BGR to RGB for MediaPipe compatibility.\n",
    "    * MediaPipe extracts the hand landmarks.\n",
    "    * The X and Y coordinates of these landmarks are isolated to create the dataset.\n",
    "\n",
    "\n",
    "* **Storage:** The processed landmark data and corresponding labels are saved into a `data.pickle` file.\n",
    "\n",
    "### **Step 3: Model Training**\n",
    "\n",
    "* **Model Choice:** A **Random Forest Classifier** is used via Scikit-learn.\n",
    "* **Data Splitting:** The dataset is split into training and testing sets, with 20% of the data reserved for testing.\n",
    "* **Best Practices:**\n",
    "    * **Shuffling:** Data is shuffled to prevent bias.\n",
    "    * **Stratification:** The `stratify` parameter ensures the training and test sets maintain the same proportion of labels (one-third for each letter).\n",
    "\n",
    "\n",
    "* **Results:** The model achieves 100% accuracy on the test set and is saved as `model.p`.\n",
    "\n",
    "### **Step 4: Real-Time Testing**\n",
    "\n",
    "* **Inference:** The system reads the webcam feed, extracts hand landmarks in real-time, and feeds them to the loaded Random Forest model.\n",
    "* **Visualization:**\n",
    "    * Prediction outputs (0, 1, 2) are mapped back to letters (A, B, L).\n",
    "    * OpenCV is used to draw a bounding box and the predicted letter text directly onto the video frame.\n",
    "\n",
    "\n",
    "* **Outcome:** The final system successfully detects and labels the hand signs in real-time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7133448",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "# **Two approaches to solving the sign language detection problem:**\n",
    "\n",
    "### **Approach 1: Image Classifier (The \"Pixel\" Approach)**\n",
    "\n",
    "* **Steps:**\n",
    "    * Take the **entire video frame** (or a specific cropped region of the hand) as the input.\n",
    "    * Feed all the raw pixels of that image into a classifier.\n",
    "    * The model tries to learn patterns from the visual data (pixels) to distinguish between classes like 'A', 'B', and 'L'.\n",
    "\n",
    "\n",
    "* **When to use:**\n",
    "    * Use this when you need to analyze visual textures or details that aren't captured by simple lines or points.\n",
    "    * *Note:* The instructor notes this approach typically works well and can achieve high accuracy, but it processes a lot of unnecessary data (like the background, lighting, or the user's face).\n",
    "\n",
    "\n",
    "\n",
    "### **Approach 2: Landmark Detector (The \"Geometry\" Approach)**\n",
    "\n",
    "* **Steps:**\n",
    "    * Use a pre-trained model (like MediaPipe) to detect specific **key points (landmarks)** on the hand, such as finger joints and tips.\n",
    "    * Discard the image itself and keep **only the X and Y coordinates** of these points.\n",
    "    * Train a classifier solely on this list of coordinates (the \"skeleton\" of the hand).\n",
    "\n",
    "\n",
    "* **When to use:**\n",
    "    * Use this when the information you need is strictly defined by **posture or shape** (e.g., finger positions in sign language or yoga poses).\n",
    "    * This is the preferred method in the video because it removes \"noise\" (background, lighting, skin color) and makes the model much smaller, faster, and more robust."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
